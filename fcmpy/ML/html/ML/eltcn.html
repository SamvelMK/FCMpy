<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ML.eltcn API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ML.eltcn</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn import datasets
from sklearn import model_selection
from sklearn.utils import shuffle
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score
from tensorflow.keras import backend as K
from tensorflow.python.keras.backend import set_session
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt
from scipy.stats import entropy
from math import log, e
import warnings
warnings.filterwarnings(&#34;ignore&#34;)


class WeightRegularizer(tf.keras.regularizers.Regularizer):

    def __init__(self, coef, mask):
        self.coef = coef
        self.mask = mask

    def __call__(self, weight_matrix):
        tensor = tf.convert_to_tensor(self.coef, np.float32)
        reg1 = K.sum(K.abs(tf.math.multiply(tf.math.subtract(weight_matrix,tensor), self.mask)))
        reg2 = 0.001 * K.sum(K.square(weight_matrix))
        return reg1 + reg2

    def get_config(self):
        return {&#39;coefficients&#39;: self.coef, &#39;penalization&#39;: self.mask}
    

def run_model(file, layers=5, folds=5, epochs=1000, verbose=True, regularize=True):
    X, y, out = read_arff(file)
    hidden = len(X[0])
    
    skf = StratifiedKFold(n_splits=folds)
    skf.get_n_splits(X, y)
    
    acc_arr = []
    ent_arr = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = np.asarray(X[train_index]).astype(np.float32),np.asarray(X[test_index]).astype(np.float32) #X[train_index], X[test_index]
        y_train, y_test = np.asarray(y[train_index]).astype(np.float32),np.asarray(y[test_index]).astype(np.float32) #y[train_index], y[test_index]
#         X = np.asarray(X).astype(np.float32)
        if(regularize):
            coef, mask = coefficients(matrix=X_train)
            network = [tf.keras.layers.Flatten()]     
    
        for i in range(layers):
            reg = None if (i == layers-1 or not regularize) else WeightRegularizer(coef, mask)
            dense = tf.keras.layers.Dense(hidden, activation=tf.nn.tanh, kernel_regularizer=reg)
            network.append(dense)

        network.append(tf.keras.layers.Dense(out, activation=tf.nn.softmax))

        model = tf.keras.models.Sequential(network)
        model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        history = model.fit(X_train, y_train, epochs=epochs, verbose=0)
        
        if(verbose):
            plot_loss_weights(history, model, mask)
        
        weights = model.get_weights()
        relajo = error(weights, coef, mask)
        ent_arr.append(relajo)
        
        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        acc_arr.append(acc)

    return np.mean(acc_arr), np.mean(ent_arr), weights 

def coefficients(matrix):
    n, m = matrix.shape
    temp1 = np.sum(matrix, axis=0)
    temp2 = np.sum(matrix**2, axis=0)
    
    df_data = pd.DataFrame(data=matrix,dtype=float)
    pearson = np.array(df_data.corr())
    
    mask = np.zeros((m, m))
    coef = np.zeros((m, m))
    for i in range(0, m):
        for j in range(0, m):
            den = n * temp2[i] - temp1[i] ** 2
            if (den != 0):
                coef[i,j] = (n * np.sum(matrix[:,i] * matrix[:,j]) - temp1[i] * temp1[j]) / den
                
                if(abs(pearson[i,j]) &gt; 0.5):
                    mask[i,j] = log(n)
 
    return coef, mask

def plot_loss_weights(history, model, mask):
    fig1, axes1 = plt.subplots(figsize=(15,5))
    fig2, axes2 = plt.subplots(figsize=(15,5))
        
    # plotting the training loss
    axes1.plot(history.history[&#39;loss&#39;])
    axes1.set_title(&#39;model loss&#39;)
    axes1.set(ylabel=&#39;loss value&#39;, xlabel=&#39;epoch&#39;)

    # plotting the weights
    weights = model.get_weights()

    data = [xi.flatten().tolist() for xi in weights[:-2:2]]
    df = pd.DataFrame(data)
    
    header = []
    for i in range(len(mask)):
        for j in range(len(mask)):
            header.append(&#39;w&#39;+str(i+1)+str(j+1)+&#34;*&#34; if (mask[i,j] != 0) else &#39;w&#39;+str(i+1)+str(j+1))
            
    df.columns = header
    import seaborn as sns
    axes2 = sns.boxplot(data=df)
    axes2.set_title(&#39;weights&#39;)
    axes2.set(ylabel=&#39;value across layers&#39;, xlabel=&#39;weight&#39;)
    
def error(weights, coef, mask):
    if(np.sum(mask) == 0):
        return 0
    
    coef = coef.flatten()
    mask = mask.flatten()
    array = []
    
    count = 0
    matrix = [xi.flatten() for xi in weights[:-2:2]]
    for i in range(len(matrix[0])):
        layer = column(matrix, i)
        for j in range(len(layer)):
            if(mask[j] &gt; 0):
                den = max(1, max(layer[j], coef[j]))
                array.append(abs(layer[j] - coef[j]) / den)
                count += 1

    return np.sum(array)/count

def column(matrix, i):
    return [row[i] for row in matrix]
    
def read_arff(file):
    # reading from .arff file
    # consistency, in the MP algorithm we take the arff without header, so we cannot req header here
    from scipy.io import arff
    data, meta = arff.loadarff(file)
    frame = pd.DataFrame(data)
    class_att = meta.names()[-1]
    y = frame[class_att]
    labels = np.unique(y)
    mapping = pd.Series([x[0] for x in enumerate(labels)], index = labels)
    y = np.array(y.map(mapping))
    X = np.array(frame)[:,0:-1]
    return X, y, len(labels)    
    
def run(path):
    import os
    files = os.listdir(path)
    print(f&#34;file in your data directory {files}. make sure they are .arff files!&#34;)
    print(&#34;running...&#34;)
    results = []
    for file in files:
        if &#34;.arff&#34; not in file:
            print(f&#34;{file} is not an .arff file&#34;)
        acc, ent, weights = run_model(path + &#34;/&#34; + file)
        results.append({&#39;acc&#39;:acc,&#39;ent&#39;:ent,&#39;weights&#39;:weights})
        print(file.replace(&#39;.arff&#39;,&#39;&#39;) + &#34;,&#34; + str(acc)+ &#34;,&#34; + str(ent))
    return results


if __name__==&#39;__main__&#39;:

    run_exp(&#39;data&#39;)
    print(&#34;Done!&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ML.eltcn.coefficients"><code class="name flex">
<span>def <span class="ident">coefficients</span></span>(<span>matrix)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def coefficients(matrix):
    n, m = matrix.shape
    temp1 = np.sum(matrix, axis=0)
    temp2 = np.sum(matrix**2, axis=0)
    
    df_data = pd.DataFrame(data=matrix,dtype=float)
    pearson = np.array(df_data.corr())
    
    mask = np.zeros((m, m))
    coef = np.zeros((m, m))
    for i in range(0, m):
        for j in range(0, m):
            den = n * temp2[i] - temp1[i] ** 2
            if (den != 0):
                coef[i,j] = (n * np.sum(matrix[:,i] * matrix[:,j]) - temp1[i] * temp1[j]) / den
                
                if(abs(pearson[i,j]) &gt; 0.5):
                    mask[i,j] = log(n)
 
    return coef, mask</code></pre>
</details>
</dd>
<dt id="ML.eltcn.column"><code class="name flex">
<span>def <span class="ident">column</span></span>(<span>matrix, i)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def column(matrix, i):
    return [row[i] for row in matrix]</code></pre>
</details>
</dd>
<dt id="ML.eltcn.error"><code class="name flex">
<span>def <span class="ident">error</span></span>(<span>weights, coef, mask)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def error(weights, coef, mask):
    if(np.sum(mask) == 0):
        return 0
    
    coef = coef.flatten()
    mask = mask.flatten()
    array = []
    
    count = 0
    matrix = [xi.flatten() for xi in weights[:-2:2]]
    for i in range(len(matrix[0])):
        layer = column(matrix, i)
        for j in range(len(layer)):
            if(mask[j] &gt; 0):
                den = max(1, max(layer[j], coef[j]))
                array.append(abs(layer[j] - coef[j]) / den)
                count += 1

    return np.sum(array)/count</code></pre>
</details>
</dd>
<dt id="ML.eltcn.plot_loss_weights"><code class="name flex">
<span>def <span class="ident">plot_loss_weights</span></span>(<span>history, model, mask)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_loss_weights(history, model, mask):
    fig1, axes1 = plt.subplots(figsize=(15,5))
    fig2, axes2 = plt.subplots(figsize=(15,5))
        
    # plotting the training loss
    axes1.plot(history.history[&#39;loss&#39;])
    axes1.set_title(&#39;model loss&#39;)
    axes1.set(ylabel=&#39;loss value&#39;, xlabel=&#39;epoch&#39;)

    # plotting the weights
    weights = model.get_weights()

    data = [xi.flatten().tolist() for xi in weights[:-2:2]]
    df = pd.DataFrame(data)
    
    header = []
    for i in range(len(mask)):
        for j in range(len(mask)):
            header.append(&#39;w&#39;+str(i+1)+str(j+1)+&#34;*&#34; if (mask[i,j] != 0) else &#39;w&#39;+str(i+1)+str(j+1))
            
    df.columns = header
    import seaborn as sns
    axes2 = sns.boxplot(data=df)
    axes2.set_title(&#39;weights&#39;)
    axes2.set(ylabel=&#39;value across layers&#39;, xlabel=&#39;weight&#39;)</code></pre>
</details>
</dd>
<dt id="ML.eltcn.read_arff"><code class="name flex">
<span>def <span class="ident">read_arff</span></span>(<span>file)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_arff(file):
    # reading from .arff file
    # consistency, in the MP algorithm we take the arff without header, so we cannot req header here
    from scipy.io import arff
    data, meta = arff.loadarff(file)
    frame = pd.DataFrame(data)
    class_att = meta.names()[-1]
    y = frame[class_att]
    labels = np.unique(y)
    mapping = pd.Series([x[0] for x in enumerate(labels)], index = labels)
    y = np.array(y.map(mapping))
    X = np.array(frame)[:,0:-1]
    return X, y, len(labels)    </code></pre>
</details>
</dd>
<dt id="ML.eltcn.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(path):
    import os
    files = os.listdir(path)
    print(f&#34;file in your data directory {files}. make sure they are .arff files!&#34;)
    print(&#34;running...&#34;)
    results = []
    for file in files:
        if &#34;.arff&#34; not in file:
            print(f&#34;{file} is not an .arff file&#34;)
        acc, ent, weights = run_model(path + &#34;/&#34; + file)
        results.append({&#39;acc&#39;:acc,&#39;ent&#39;:ent,&#39;weights&#39;:weights})
        print(file.replace(&#39;.arff&#39;,&#39;&#39;) + &#34;,&#34; + str(acc)+ &#34;,&#34; + str(ent))
    return results</code></pre>
</details>
</dd>
<dt id="ML.eltcn.run_model"><code class="name flex">
<span>def <span class="ident">run_model</span></span>(<span>file, layers=5, folds=5, epochs=1000, verbose=True, regularize=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_model(file, layers=5, folds=5, epochs=1000, verbose=True, regularize=True):
    X, y, out = read_arff(file)
    hidden = len(X[0])
    
    skf = StratifiedKFold(n_splits=folds)
    skf.get_n_splits(X, y)
    
    acc_arr = []
    ent_arr = []
    for train_index, test_index in skf.split(X, y):
        X_train, X_test = np.asarray(X[train_index]).astype(np.float32),np.asarray(X[test_index]).astype(np.float32) #X[train_index], X[test_index]
        y_train, y_test = np.asarray(y[train_index]).astype(np.float32),np.asarray(y[test_index]).astype(np.float32) #y[train_index], y[test_index]
#         X = np.asarray(X).astype(np.float32)
        if(regularize):
            coef, mask = coefficients(matrix=X_train)
            network = [tf.keras.layers.Flatten()]     
    
        for i in range(layers):
            reg = None if (i == layers-1 or not regularize) else WeightRegularizer(coef, mask)
            dense = tf.keras.layers.Dense(hidden, activation=tf.nn.tanh, kernel_regularizer=reg)
            network.append(dense)

        network.append(tf.keras.layers.Dense(out, activation=tf.nn.softmax))

        model = tf.keras.models.Sequential(network)
        model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        history = model.fit(X_train, y_train, epochs=epochs, verbose=0)
        
        if(verbose):
            plot_loss_weights(history, model, mask)
        
        weights = model.get_weights()
        relajo = error(weights, coef, mask)
        ent_arr.append(relajo)
        
        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        acc_arr.append(acc)

    return np.mean(acc_arr), np.mean(ent_arr), weights </code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ML.eltcn.WeightRegularizer"><code class="flex name class">
<span>class <span class="ident">WeightRegularizer</span></span>
<span>(</span><span>coef, mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Regularizer base class.</p>
<p>Regularizers allow you to apply penalties on layer parameters or layer
activity during optimization. These penalties are summed into the loss
function that the network optimizes.</p>
<p>Regularization penalties are applied on a per-layer basis. The exact API will
depend on the layer, but many layers (e.g. <code>Dense</code>, <code>Conv1D</code>, <code>Conv2D</code> and
<code>Conv3D</code>) have a unified API.</p>
<p>These layers expose 3 keyword arguments:</p>
<ul>
<li><code>kernel_regularizer</code>: Regularizer to apply a penalty on the layer's kernel</li>
<li><code>bias_regularizer</code>: Regularizer to apply a penalty on the layer's bias</li>
<li><code>activity_regularizer</code>: Regularizer to apply a penalty on the layer's output</li>
</ul>
<p>All layers (including custom layers) expose <code>activity_regularizer</code> as a
settable property, whether or not it is in the constructor arguments.</p>
<p>The value returned by the <code>activity_regularizer</code> is divided by the input
batch size so that the relative weighting between the weight regularizers and
the activity regularizers does not change with the batch size.</p>
<p>You can access a layer's regularization penalties by calling <code>layer.losses</code>
after calling the layer on inputs.</p>
<h2 id="example">Example</h2>
<pre><code class="python-repl">&gt;&gt;&gt; layer = tf.keras.layers.Dense(
...     5, input_dim=5,
...     kernel_initializer='ones',
...     kernel_regularizer=tf.keras.regularizers.l1(0.01),
...     activity_regularizer=tf.keras.regularizers.l2(0.01))
&gt;&gt;&gt; tensor = tf.ones(shape=(5, 5)) * 2.0
&gt;&gt;&gt; out = layer(tensor)
</code></pre>
<pre><code class="python-repl">&gt;&gt;&gt; # The kernel regularization term is 0.25
&gt;&gt;&gt; # The activity regularization term (after dividing by the batch size) is 5
&gt;&gt;&gt; tf.math.reduce_sum(layer.losses)
&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.25&gt;
</code></pre>
<h2 id="available-penalties">Available penalties</h2>
<pre><code class="python">tf.keras.regularizers.l1(0.3)  # L1 Regularization Penalty
tf.keras.regularizers.l2(0.1)  # L2 Regularization Penalty
tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)  # L1 + L2 penalties
</code></pre>
<h2 id="directly-calling-a-regularizer">Directly calling a regularizer</h2>
<p>Compute a regularization loss on a tensor by directly calling a regularizer
as if it is a one-argument function.</p>
<p>E.g.</p>
<pre><code class="python-repl">&gt;&gt;&gt; regularizer = tf.keras.regularizers.l2(2.)
&gt;&gt;&gt; tensor = tf.ones(shape=(5, 5))
&gt;&gt;&gt; regularizer(tensor)
&lt;tf.Tensor: shape=(), dtype=float32, numpy=50.0&gt;
</code></pre>
<h2 id="developing-new-regularizers">Developing new regularizers</h2>
<p>Any function that takes in a weight matrix and returns a scalar
tensor can be used as a regularizer, e.g.:</p>
<pre><code class="python-repl">&gt;&gt;&gt; @tf.keras.utils.register_keras_serializable(package='Custom', name='l1')
... def l1_reg(weight_matrix):
...    return 0.01 * tf.math.reduce_sum(tf.math.abs(weight_matrix))
...
&gt;&gt;&gt; layer = tf.keras.layers.Dense(5, input_dim=5,
...     kernel_initializer='ones', kernel_regularizer=l1_reg)
&gt;&gt;&gt; tensor = tf.ones(shape=(5, 5))
&gt;&gt;&gt; out = layer(tensor)
&gt;&gt;&gt; layer.losses
[&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.25&gt;]
</code></pre>
<p>Alternatively, you can write your custom regularizers in an
object-oriented way by extending this regularizer base class, e.g.:</p>
<pre><code class="python-repl">&gt;&gt;&gt; @tf.keras.utils.register_keras_serializable(package='Custom', name='l2')
... class L2Regularizer(tf.keras.regularizers.Regularizer):
...   def __init__(self, l2=0.):  # pylint: disable=redefined-outer-name
...     self.l2 = l2
...
...   def __call__(self, x):
...     return self.l2 * tf.math.reduce_sum(tf.math.square(x))
...
...   def get_config(self):
...     return {'l2': float(self.l2)}
...
&gt;&gt;&gt; layer = tf.keras.layers.Dense(
...   5, input_dim=5, kernel_initializer='ones',
...   kernel_regularizer=L2Regularizer(l2=0.5))
</code></pre>
<pre><code class="python-repl">&gt;&gt;&gt; tensor = tf.ones(shape=(5, 5))
&gt;&gt;&gt; out = layer(tensor)
&gt;&gt;&gt; layer.losses
[&lt;tf.Tensor: shape=(), dtype=float32, numpy=12.5&gt;]
</code></pre>
<h3 id="a-note-on-serialization-and-deserialization">A note on serialization and deserialization:</h3>
<p>Registering the regularizers as serializable is optional if you are just
training and executing models, exporting to and from SavedModels, or saving
and loading weight checkpoints.</p>
<p>Registration is required for Keras <code>model_to_estimator</code>, saving and
loading models to HDF5 formats, Keras model cloning, some visualization
utilities, and exporting models to and from JSON. If using this functionality,
you must make sure any python process running your model has also defined
and registered your custom regularizer.</p>
<p><code>tf.keras.utils.register_keras_serializable</code> is only available in TF 2.1 and
beyond. In earlier versions of TensorFlow you must pass your custom
regularizer to the <code>custom_objects</code> argument of methods that expect custom
regularizers to be registered as serializable.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WeightRegularizer(tf.keras.regularizers.Regularizer):

    def __init__(self, coef, mask):
        self.coef = coef
        self.mask = mask

    def __call__(self, weight_matrix):
        tensor = tf.convert_to_tensor(self.coef, np.float32)
        reg1 = K.sum(K.abs(tf.math.multiply(tf.math.subtract(weight_matrix,tensor), self.mask)))
        reg2 = 0.001 * K.sum(K.square(weight_matrix))
        return reg1 + reg2

    def get_config(self):
        return {&#39;coefficients&#39;: self.coef, &#39;penalization&#39;: self.mask}</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>tensorflow.python.keras.regularizers.Regularizer</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="ML.eltcn.WeightRegularizer.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the config of the regularizer.</p>
<p>An regularizer config is a Python dictionary (serializable)
containing all configuration parameters of the regularizer.
The same regularizer can be reinstantiated later
(without any saved state) from this configuration.</p>
<p>This method is optional if you are just training and executing models,
exporting to and from SavedModels, or using weight checkpoints.</p>
<p>This method is required for Keras <code>model_to_estimator</code>, saving and
loading models to HDF5 formats, Keras model cloning, some visualization
utilities, and exporting models to and from JSON.</p>
<h2 id="returns">Returns</h2>
<p>Python dictionary.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config(self):
    return {&#39;coefficients&#39;: self.coef, &#39;penalization&#39;: self.mask}</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ML" href="index.html">ML</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="ML.eltcn.coefficients" href="#ML.eltcn.coefficients">coefficients</a></code></li>
<li><code><a title="ML.eltcn.column" href="#ML.eltcn.column">column</a></code></li>
<li><code><a title="ML.eltcn.error" href="#ML.eltcn.error">error</a></code></li>
<li><code><a title="ML.eltcn.plot_loss_weights" href="#ML.eltcn.plot_loss_weights">plot_loss_weights</a></code></li>
<li><code><a title="ML.eltcn.read_arff" href="#ML.eltcn.read_arff">read_arff</a></code></li>
<li><code><a title="ML.eltcn.run" href="#ML.eltcn.run">run</a></code></li>
<li><code><a title="ML.eltcn.run_model" href="#ML.eltcn.run_model">run_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ML.eltcn.WeightRegularizer" href="#ML.eltcn.WeightRegularizer">WeightRegularizer</a></code></h4>
<ul class="">
<li><code><a title="ML.eltcn.WeightRegularizer.get_config" href="#ML.eltcn.WeightRegularizer.get_config">get_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>